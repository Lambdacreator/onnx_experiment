{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regressionn convertor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from onnx.helper import (make_model, make_node, make_graph, make_tensor_value_info)\n",
    "from onnx import numpy_helper, TensorProto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train the data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X = X.astype(np.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_to_onnx(model): \n",
    "    # initializer\n",
    "    A = numpy_helper.from_array(model.coef_, name = 'A') # coefficient\n",
    "    B = numpy_helper.from_array(model.intercept_, name = 'B') # intercept\n",
    "    \n",
    "    # inputs\n",
    "    X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n",
    "\n",
    "    # outputs\n",
    "    Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n",
    "\n",
    "    # nodes\n",
    "    node1 = make_node('MatMul', ['X', 'A'], ['XA'])\n",
    "    node2 = make_node('Add', ['XA', 'B'], ['Y'])\n",
    "\n",
    "    graph = make_graph([node1, node2],  # nodes\n",
    "                        'lr',  # name\n",
    "                        [X], # input\n",
    "                        [Y], # output\n",
    "                        [A, B]) # initializer\n",
    "\n",
    "    # create onnx model    \n",
    "    onnx_model = make_model(graph)\n",
    "    \n",
    "    # version controll\n",
    "    onnx_model.ir_version = 9\n",
    "    del onnx_model.opset_import[:]\n",
    "    opset = onnx_model.opset_import.add()\n",
    "    opset.version = 20\n",
    "\n",
    "    return onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = linear_to_onnx(model)\n",
    "\n",
    "with open(\"linear_regression.onnx\", \"wb\") as f:\n",
    "     f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the prediction with onnxruntime.\n",
    "import onnxruntime as rt\n",
    "\n",
    "sess = rt.InferenceSession('linear_regression.onnx', providers=[\"CPUExecutionProvider\"])\n",
    "sess.run(None, {\"X\": X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Path to the ONNX model file\n",
    "    onnx_file_path = \"linear_model.onnx\"\n",
    "    \n",
    "    # Example input data (replace with actual data)\n",
    "    input_data = np.array([[10]], dtype=np.float32)\n",
    "    \n",
    "    # Create an inference session\n",
    "    session = create_onnx_session(onnx_file_path)\n",
    "    \n",
    "    # Perform inference\n",
    "    result = linear_regression_inference(session, input_data)\n",
    "    print(\"Inference result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create own backend runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "# load ONNX model\n",
    "linear_model = onnx.load(\"linear_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx2tf\n",
    "\n",
    "# onnx2tf.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 0\n",
    "operators = []\n",
    "\n",
    "\n",
    "model_name = linear_model\n",
    "\n",
    "\n",
    "while start != -1:\n",
    "    start = str(model_name.graph.node).find('op_type: \"', end)\n",
    "    end = str(model_name.graph.node).find('\\n', start)\n",
    "    operators.append(str(model_name.graph.node)[start:end][10:-1])\n",
    "\n",
    "\n",
    "operators != operators.pop()\n",
    "operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get onnx operators\n",
    "\n",
    "start = 0\n",
    "end = 0\n",
    "operators = []\n",
    "\n",
    "model_name = linear_model\n",
    "\n",
    "while start != -1: \n",
    "    start = str(model_name.graph.node).find('op_type: \"', end)\n",
    "    end = str(model_name.graph.node).find('\\n', start)\n",
    "    operators.append(str(model_name.graph.node)[start:end][10:-1])\n",
    "\n",
    "operators != operators.pop()\n",
    "operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping keys to functions\n",
    "\n",
    "def MatMul(n, m):\n",
    "    return np.dot(n, m)\n",
    "\n",
    "def Add(n, m):\n",
    "    return n+m\n",
    "\n",
    "function_dict = {\n",
    "    \"MatMul\": MatMul,\n",
    "    \"Add\": Add\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx_run(model, input):\n",
    "    # get onnx operators\n",
    "\n",
    "    start = 0\n",
    "    end = 0\n",
    "    operators = []\n",
    "\n",
    "    model_name = model\n",
    "\n",
    "    while start != -1: \n",
    "        start = str(model_name.graph.node).find('op_type: \"', end)\n",
    "        end = str(model_name.graph.node).find('\\n', start)\n",
    "        operators.append(str(model_name.graph.node)[start:end][10:-1])\n",
    "\n",
    "    operators != operators.pop()\n",
    "\n",
    "    # run operators\n",
    "\n",
    "    from onnx import numpy_helper\n",
    "\n",
    "    result = input\n",
    "\n",
    "    for i in range(len(operators)): \n",
    "        result = function_dict[operators[i]](result, numpy_helper.to_array(model.graph.initializer[i]))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_run(linear_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model with onnx backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx.backend.base import Backend\n",
    "\n",
    "\n",
    "Backend.run_model(model = onnx_model, inputs = [6.4, 2.7, 5.3, 1.9]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# initializer\n",
    "value = np.array([1/255], dtype = np.float32)\n",
    "rescale_unit = numpy_helper.from_array(value, name = 'rescale_unit')\n",
    "\n",
    "reshape_value = np.array([-1, 30976], dtype = np.float32)\n",
    "reshape_value = numpy_helper.from_array(reshape_value, name = 'reshape_value')\n",
    "\n",
    "# tensor value\n",
    "input = make_tensor_value_info('input', \n",
    "                               TensorProto.FLOAT, \n",
    "                               [1, 180, 180, 3])\n",
    "\n",
    "output = make_tensor_value_info('output', \n",
    "                                TensorProto.FLOAT, \n",
    "                                [None])\n",
    "\n",
    "# nodes\n",
    "rescale_node = make_node('MatMul', ['input', 'rescale_unit'], ['rescaled_input'])\n",
    "\n",
    "transpose_node = make_node('Transpose', ['rescaled_input'], ['transpose_input'], perm = [0, 3, 1, 2])\n",
    "# permutation: [1, 180, 180, 3] ---(perm = [0, 3, 1, 2])--> [1, 3, 180, 180]\n",
    "\n",
    "conv_node = make_node(\n",
    "        'Conv',\n",
    "        ['transpose_input', 'weights'],\n",
    "        ['conv_output'],\n",
    "        name = 'conv',\n",
    "        kernel_shape = [3, 3],\n",
    "        strides = [1, 1],\n",
    "        pads = [1, 1, 1, 1]\n",
    "        )\n",
    "\n",
    "relu_node = make_node('Relu', ['conv_output'], ['relu_output'])\n",
    "\n",
    "maxpool_node = make_node(\n",
    "        'MaxPool', \n",
    "        ['relu_output'], \n",
    "        ['maxpool_output'], \n",
    "        name = 'maxpool', \n",
    "        kernel = [2, 2], \n",
    "        strides = [2, 2]\n",
    "    )\n",
    "\n",
    "reshape_node = make_node(\n",
    "        'Reshape', \n",
    "        ['maxpool_output', 'reshape_value'], \n",
    "        ['output']\n",
    ")\n",
    "\n",
    "# create graph\n",
    "graph = make_graph([rescale_node, transpose_node, conv_node, relu_node, maxpool_node, reshape_node], \n",
    "                   'cnn', \n",
    "                   [input], \n",
    "                   [output], \n",
    "                   [rescale_unit, reshape_value]\n",
    "                   )\n",
    "\n",
    "onnx_model = make_model(graph)\n",
    "with open(\"naive_cnn.onnx\", \"wb\") as f:\n",
    "     f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_too_onnx(model, input_shape): \n",
    "    # input \n",
    "    input = make_tensor_value_info('input', \n",
    "                                   TensorProto.FLOAT, \n",
    "                                   input_shape)\n",
    "\n",
    "    # output\n",
    "    output = make_tensor_value_info('output', \n",
    "                                    TensorProto.FLOAT, \n",
    "                                    [None])\n",
    "    \n",
    "    # create node for convolutional filter\n",
    "    conv_node = make_node(\n",
    "        'Conv',\n",
    "        ['input', 'weights'],\n",
    "        ['conv_output'],\n",
    "        name = 'conv',\n",
    "        kernel_shape = [3, 3],\n",
    "        strides = [1, 1],\n",
    "        pads = [1, 1, 1, 1]\n",
    "        )\n",
    "    \n",
    "    # create node for relu activation\n",
    "    relu_node = make_node(\n",
    "        'Relu', \n",
    "        ['conv_output'], \n",
    "        ['relu_output'], \n",
    "        name = 'relu'\n",
    "    )\n",
    "    \n",
    "    # create node for max pooling\n",
    "    maxpool_node = make_node(\n",
    "        'MaxPool', \n",
    "        ['relu_output'], \n",
    "        ['output'], \n",
    "        name = 'maxpool', \n",
    "        kernel = [2, 2], \n",
    "        strides = [2, 2]\n",
    "    )\n",
    "\n",
    "    graph = make_graph(\n",
    "        [conv_node, relu_node, maxpool_node], \n",
    "        'cnn_model', \n",
    "        [input], \n",
    "        [output]\n",
    "    )\n",
    "\n",
    "    onnx_model = make_model(graph)\n",
    "\n",
    "    # version controll\n",
    "    onnx_model.ir_version = 9\n",
    "    del onnx_model.opset_import[:]\n",
    "    opset = onnx_model.opset_import.add()\n",
    "    opset.version = 20\n",
    "    \n",
    "    return onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_onnx = cnn_too_onnx()\n",
    "onnx.save(cnn_onnx, 'simple_cnn.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "simple_cnn = onnx.load('sequential_2.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 0\n",
    "operators = []\n",
    "\n",
    "model_name = simple_cnn\n",
    "\n",
    "while start != -1: \n",
    "    start = str(model_name.graph.node).find('op_type: \"', end)\n",
    "    end = str(model_name.graph.node).find('\\n', start)\n",
    "    operators.append(str(model_name.graph.node)[start:end][10:-1])\n",
    "\n",
    "operators != operators.pop()\n",
    "operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.graph.initializer[2], numpy_helper.to_array(simple_cnn.graph.initializer[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "flower_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Bridal_pink_-_morwell_rose_garden.jpg/800px-Bridal_pink_-_morwell_rose_garden.jpg\"\n",
    "flower_path = tf.keras.utils.get_file(origin = flower_url)\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    flower_path, target_size = (180, 180)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = function_dict['MatMul'](img_array, numpy_helper.to_array(simple_cnn.graph.initializer[1]))\n",
    "result2 = function_dict['Add'](result1, numpy_helper.to_array(simple_cnn.graph.initializer[0]))\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping keys to functions\n",
    "\n",
    "def MatMul(input, m):\n",
    "    return np.dot(input, m)\n",
    "\n",
    "def Add(input, m):\n",
    "    return input+m\n",
    "\n",
    "def Transpose(input): \n",
    "    transposed_matrix = [[input[j][i] for j in range(len(input))] for i in range(len(input[0]))]\n",
    "    return transposed_matrix\n",
    "\n",
    "def Convolutional(input, kernel): \n",
    "    m, _ = kernel.shape\n",
    "\n",
    "    y, x = input.shape\n",
    "    y = y - m + 1\n",
    "    x = x - m + 1\n",
    "\n",
    "    output_layer = np.zeros((y,x))\n",
    "\n",
    "    for i in range(y):\n",
    "        for j in range(x):\n",
    "            output_layer[i][j] = np.sum(input[i:i + m, j:j + m] * kernel)\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "def Relu(input):\n",
    "    return np.maximum(0, input)\n",
    "\n",
    "def MaxPool(input, pool_size):\n",
    "\n",
    "    m, n = input.shape\n",
    "    pool_height, pool_width = pool_size\n",
    "\n",
    "    output_height = m // pool_height\n",
    "    output_width = n // pool_width\n",
    "    output_layer = np.zeros((output_height, output_width))\n",
    "\n",
    "    for i in range(output_height):\n",
    "        for j in range(output_width):\n",
    "            start_i = i * pool_height\n",
    "            start_j = j * pool_width\n",
    "            max_val = np.max(input[start_i:start_i + pool_height,\n",
    "                                         start_j:start_j + pool_width])\n",
    "            output_layer[i, j] = max_val\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "def Reshape(array, new_shape):\n",
    "\n",
    "    new_size = 1\n",
    "    for dim in new_shape:\n",
    "        new_size *= dim\n",
    "    \n",
    "    reshaped_array = []\n",
    "    current_index = 0\n",
    "    \n",
    "    for i in range(new_shape[0]):\n",
    "        row = []\n",
    "        for j in range(new_shape[1]):\n",
    "            row.append(array[current_index])\n",
    "            current_index += 1\n",
    "        reshaped_array.append(row)\n",
    "    \n",
    "    return reshaped_array\n",
    "\n",
    "def Gemm(input, weight, bias):\n",
    "    return np.dot(input, weight) + bias\n",
    "    \n",
    "function_dict = {\n",
    "    \"MatMul\": MatMul,\n",
    "    \"Add\": Add, \n",
    "    \"Transpose\": Transpose, \n",
    "    \"Conv\": Convolutional, \n",
    "    \"Relu\": Relu, \n",
    "    \"MAxPool\": MaxPool, \n",
    "    \"Reshape\": Reshape, \n",
    "    \"Gemm\": Gemm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(180, 180, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators = []\n",
    "\n",
    "for layer in model.layers: \n",
    "    operators.append(layer._name)\n",
    "\n",
    "operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest convertor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import helper\n",
    "\n",
    "# Define a function to create a single decision tree node\n",
    "def make_decision_tree_node(tree_index, feature_index, threshold, left_child, right_child, leaf_value):\n",
    "    node_inputs = [\n",
    "        f'input_{tree_index}', f'feature_index_{tree_index}', f'threshold_{tree_index}',\n",
    "        f'left_child_{tree_index}', f'right_child_{tree_index}', f'leaf_value_{tree_index}'\n",
    "    ]\n",
    "    \n",
    "    node_outputs = [f'output_{tree_index}']\n",
    "    \n",
    "    return helper.make_node(\n",
    "        'DecisionTree',\n",
    "        node_inputs,\n",
    "        node_outputs,\n",
    "        name=f'decision_tree_{tree_index}',\n",
    "        domain=\"ai.onnx.ml\"\n",
    "    )\n",
    "\n",
    "# Define the random forest model\n",
    "def make_model(num_trees, num_features):\n",
    "    graph_inputs = [\n",
    "        helper.make_tensor_value_info('input', onnx.TensorProto.FLOAT, (None, num_features))\n",
    "    ]\n",
    "    \n",
    "    graph_outputs = [\n",
    "        helper.make_tensor_value_info('output', onnx.TensorProto.FLOAT, (None,))\n",
    "    ]\n",
    "    \n",
    "    nodes = []\n",
    "    for i in range(num_trees):\n",
    "        # Example decision tree parameters (replace with your actual parameters)\n",
    "        feature_index = np.random.randint(0, num_features)\n",
    "        threshold = np.random.rand()\n",
    "        left_child = np.random.randint(-1, num_features)\n",
    "        right_child = np.random.randint(-1, num_features)\n",
    "        leaf_value = np.random.rand()\n",
    "        \n",
    "        nodes.append(\n",
    "            make_decision_tree_node(i, feature_index, threshold, left_child, right_child, leaf_value)\n",
    "        )\n",
    "    \n",
    "    return helper.make_model(\n",
    "        helper.make_graph(\n",
    "            nodes,\n",
    "            'random_forest_model',\n",
    "            graph_inputs,\n",
    "            graph_outputs\n",
    "        ),\n",
    "        producer_name='random_forest'\n",
    "    )\n",
    "\n",
    "# Create the model\n",
    "num_trees = 5\n",
    "num_features = 10\n",
    "model = make_model(num_trees, num_features)\n",
    "\n",
    "# Save the model to an ONNX file\n",
    "onnx_path = \"random_forest.onnx\"\n",
    "onnx.save(model, onnx_path)\n",
    "print(f\"Model exported to {onnx_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
